{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38d1392-0f3a-4a43-a5e1-40003be06950",
   "metadata": {},
   "source": [
    "DN: Take Home Assignment\n",
    "Date Range: 2020-2023\n",
    "\r\n",
    "Ingestion\r\n",
    "- Ingest Data\r\n",
    "  - XML,CSV-bar,JSON\r\n",
    "- Create Ticket Fee refence te\n",
    "eEDAable\r\n",
    "- Identify Police Officers in data\r\n",
    "- Identify Officer with most speeding tickets issued\r\n",
    "- What 3 months had the most speeding tickets?\r\n",
    "- Are there any trends\r\n",
    " - Monthly\r\n",
    " - Yearly\r\n",
    "-Top 10 people who have spent the most on speeding tickets\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56db91ac-5221-4cb5-9cfd-20d788acba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement spark-xml (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for spark-xml\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install spark-xml\n",
    "# !pip install delta-spark\n",
    "\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7f7553ac-c85d-4de3-ab75-f66ded1fa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML is not included in the Python release it seems, only Scala\n",
    "# https://github.com/databricks/spark-xml\n",
    "from pyspark.sql.column import Column, _to_java_column\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.utils import *\n",
    "\n",
    "def ext_from_xml(xml_column, schema, options={}):\n",
    "    java_column = _to_java_column(xml_column.cast('string'))\n",
    "    java_schema = spark._jsparkSession.parseDataType(schema.json())\n",
    "    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)\n",
    "    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(\n",
    "        java_column, java_schema, scala_map)\n",
    "    return Column(jc)\n",
    "\n",
    "def ext_schema_of_xml_df(df, options={}):\n",
    "    assert len(df.columns) == 1\n",
    "\n",
    "    # scala_options = spark._jvm.PythonUtils.toScalaMap(options)\n",
    "    scala_options = {}\n",
    "    \n",
    "    java_xml_module = getattr(getattr(\n",
    "        spark._jvm.com.databricks.spark.xml, \"package$\"), \"MODULE$\")\n",
    "    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)\n",
    "    return _parse_datatype_json_string(java_schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1cc65cdc-d281-436d-8a8b-7b610e7539b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ticket_fees = [{'type':'base', 'value':30},\n",
    "               {'type':'base + school zone','value': 60},\n",
    "               {'type':'base + construction work zone','value':60},\n",
    "               {'type':'base + school zone + construction work zone', 'value':120}]\n",
    "\n",
    "keywords = ['automobiles', 'people', 'speeding_tickets']\n",
    "\n",
    "path = './work/ttpd_data'\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e0de0147-cc45-4354-8480-5e349941b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_spark_session_() -> SparkSession:\n",
    "    builder = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('takehome') \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.16.0\") \n",
    "\n",
    "    return configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "def _read_file_(spark, file_path):\n",
    "    \n",
    "    if file_path.endswith('.xml'):\n",
    "        print(f'Reading XML file {file_path}')\n",
    "        xml = spark.read.text(file_path)\n",
    "        \n",
    "        # https://medium.com/@pawankumarshukla_57258/spark-structured-streaming-read-and-process-data-xml-and-json-from-azure-event-hub-4030567cff44\n",
    "        df_schema = spark.read.format(\"binaryFile\").load(file_path).selectExpr(\"CAST(content AS STRING)\")\n",
    "        xml_extraction_options = {\"mode\": \"DROPMALFORMED\",\"inferSchema\":\"true\"}\n",
    "        \n",
    "        xml_schema = ext_schema_of_xml_df(df_schema.select(\"content\"))\n",
    "        \n",
    "        # df=df.select(ext_from_xml(col(\"body\"), xmlschema, xml_extraction_options).alias(\"xmldf\"))\n",
    "        # schema = ext_schema_of_xml_df(xml.select(\"value\"))\n",
    "        df_schema.show()\n",
    "\n",
    "        df =[]\n",
    "        return df\n",
    "\n",
    "    elif file_path.endswith('.json'):\n",
    "        # print(f'Reading JSON file {file_path}')\n",
    "        df = spark.read.json(file_path)\n",
    "        if 'speeding_tickets' in df.columns:\n",
    "            df = df.select(explode(col('speeding_tickets')).alias('ticket')).select('ticket.*')\n",
    "        return df\n",
    "        \n",
    "    elif file_path.endswith('.csv'):\n",
    "        # print(f'Reading CSV file {file_path}')\n",
    "        return spark.read.csv(file_path, header=True, inferSchema=True,sep='|')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _combine_files_(spark, keyword):\n",
    "    filtered_files = [f for f in all_files if keyword in f]\n",
    "    if not filtered_files:\n",
    "        return None\n",
    "\n",
    "    dataframes = [_read_file_(spark, f) for f in filtered_files if _read_file_(spark, f) is not None]\n",
    "    if not dataframes:\n",
    "        return None\n",
    "\n",
    "    combined_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = _get_spark_session_()\n",
    "    print(f'Spark Session Initialized...')\n",
    "\n",
    "    combined_dfs = {keyword: _combine_files_(spark, keyword) for keyword in keywords}\n",
    "\n",
    "    for keyword, df in combined_dfs.items():\n",
    "        if df is not None:\n",
    "            print(f\"Combined DataFrame for {keyword}:\")\n",
    "            df.show()\n",
    "        else:\n",
    "            print(f\"No files found for keyword: {keyword}\")\n",
    "    fee_df = spark.createDataFrame(ticket_fees)\n",
    "    fee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60c846-7743-4e78-9038-6ef6f0fa13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "\n",
    "spark = _get_spark_session_()\n",
    "_read_file_(spark, './work/ttpd_data/20240503111609_automobiles_41.xml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
